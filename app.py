import os
import re
import time
import math
import json
import requests
import streamlit as st
import pandas as pd
import numpy as np
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from datetime import datetime, timedelta, timezone

# -----------------------------
# í™˜ê²½ ì„¤ì •
# -----------------------------
st.set_page_config(page_title="YouTube í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ê¸°", page_icon="ğŸ“ˆ", layout="wide")
API_KEY = st.secrets.get("YOUTUBE_API_KEY", os.getenv("YOUTUBE_API_KEY", ""))

BASE_URL = "https://www.googleapis.com/youtube/v3"
KST = timezone(timedelta(hours=9))
DEFAULT_STOPWORDS = {
    # í•œêµ­ì–´/ì˜ì–´ ê³µí†µ ë¶ˆìš©ì–´(í•„ìš” ì‹œ ì¶”ê°€)
    "ì˜ìƒ","ë™ì˜ìƒ","ë¸Œì´ë¡œê·¸","vlog","the","a","to","of","in","on","for","and","is","are","with","from",
    "í•œ","ê²ƒ","ìˆ˜","ì´","ê·¸","ì €","ë°","ë“±","ë•Œ","ë•Œë¬¸","ì˜¤ëŠ˜","í•˜ë£¨","ì¼ìƒ","ì±„ë„","ìœ íŠœë¸Œ","youtube",
    "shorts","short","ê³µì‹","official","full","2023","2024","2025"
}

# í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ë§Œ ë‚¨ê¸°ê¸° ìœ„í•œ ê°„ë‹¨ í† í¬ë‚˜ì´ì €
TOKEN_PATTERN = re.compile(r"[A-Za-zê°€-í£]+")

# -----------------------------
# ìœ í‹¸
# -----------------------------
def yt_get(path, params, sleep=0.0):
    """YouTube Data API GET ë˜í¼(ê°„ë‹¨ ì¿¼í„°/ì†ë„ ì œì–´)."""
    if not API_KEY:
        raise RuntimeError("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. .streamlit/secrets.toml ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ YOUTUBE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    params = dict(params)
    params["key"] = API_KEY
    url = f"{BASE_URL}/{path}"
    r = requests.get(url, params=params, timeout=30)
    if sleep:
        time.sleep(sleep)
    if r.status_code != 200:
        raise RuntimeError(f"API ì˜¤ë¥˜ {r.status_code}: {r.text}")
    return r.json()

def youtube_search(keyword, published_after, published_before, region_code, max_results=50):
    """
    ê²€ìƒ‰ ê²°ê³¼ì—ì„œ videoIdë“¤ ìˆ˜ì§‘. (search.list)
    """
    ids = []
    page_token = None
    fetched = 0
    while fetched < max_results:
        page_size = min(50, max_results - fetched)
        data = yt_get(
            "search",
            {
                "part": "id",
                "type": "video",
                "q": keyword,
                "publishedAfter": published_after,
                "publishedBefore": published_before,
                "regionCode": region_code or "",
                "maxResults": page_size,
                "order": "relevance",
            },
            sleep=0.05,
        )
        for item in data.get("items", []):
            vid = item.get("id", {}).get("videoId")
            if vid:
                ids.append(vid)
        fetched += len(data.get("items", []))
        page_token = data.get("nextPageToken")
        if not page_token or len(data.get("items", [])) == 0:
            break
        # ë‹¤ìŒ í˜ì´ì§€ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì“°ê³  ì‹¶ë‹¤ë©´ paramsì— pageToken ì¶”ê°€ (ì—¬ê¸´ relevance ìš°ì„ ì´ë¼ ìƒëµ)

    return list(dict.fromkeys(ids))  # ì¤‘ë³µ ì œê±°

def youtube_videos_stats(video_ids):
    """
    videos.listë¡œ ë©”íƒ€ë°ì´í„°/í†µê³„ë¥¼ ì¡°íšŒ.
    """
    rows = []
    # 50ê°œì”© ëŠì–´ì„œ ì¡°íšŒ
    for i in range(0, len(video_ids), 50):
        chunk = ",".join(video_ids[i:i+50])
        data = yt_get(
            "videos",
            {
                "part": "snippet,statistics,contentDetails",
                "id": chunk,
            },
            sleep=0.05,
        )
        for it in data.get("items", []):
            s = it.get("snippet", {})
            stats = it.get("statistics", {})
            row = {
                "videoId": it.get("id"),
                "title": s.get("title",""),
                "description": s.get("description",""),
                "channelTitle": s.get("channelTitle",""),
                "publishedAt": s.get("publishedAt",""),
                "viewCount": int(stats.get("viewCount", 0) or 0),
                "likeCount": int(stats.get("likeCount", 0) or 0),  # ì¼ë¶€ ì±„ë„ì€ 0 ë˜ëŠ” ë¹„ê³µê°œ
                "commentCount": int(stats.get("commentCount", 0) or 0),
            }
            rows.append(row)
    df = pd.DataFrame(rows)
    if not df.empty:
        df["publishedAt"] = pd.to_datetime(df["publishedAt"])
        df["ER(%)"] = np.where(
            df["viewCount"] > 0,
            (df["likeCount"] + df["commentCount"]) / df["viewCount"] * 100.0,
            0.0,
        ).round(3)
    return df

def tokenize(text):
    tokens = TOKEN_PATTERN.findall(text.lower())
    return [t for t in tokens if t not in DEFAULT_STOPWORDS and len(t) >= 2]

def keywords_from_df(df, topn=100):
    corpus = []
    for _, row in df.iterrows():
        corpus.extend(tokenize(row.get("title","") + " " + row.get("description","")))
    counter = Counter(corpus)
    return counter.most_common(topn)

def draw_wordcloud(freqs, font_path=None):
    wc = WordCloud(
        width=900, height=500,
        background_color="white",
        font_path=font_path, # í•œê¸€ í°íŠ¸ ê²½ë¡œ ì§€ì • í•„ìš” ì‹œ
        collocations=False
    )
    wc.generate_from_frequencies(dict(freqs))
    fig = plt.figure(figsize=(9,5))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    return fig

# -----------------------------
# UI
# -----------------------------
st.title("ğŸ“ˆ ìœ íŠœë¸Œ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ê¸°")
st.caption("í‚¤ì›Œë“œ/ê¸°ê°„ìœ¼ë¡œ ìƒìœ„ ë™ì˜ìƒì„ ìˆ˜ì§‘í•´ ì›Œë“œí´ë¼ìš°ë“œì™€ ì°¸ì—¬ìœ¨(ER)ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.subheader("ê²€ìƒ‰ ì„¤ì •")
    keyword = st.text_input("í‚¤ì›Œë“œ (ì˜ˆ: ë¸Œì´ë¡œê·¸ / ì˜ì–´ ê°€ëŠ¥)", "ë¸Œì´ë¡œê·¸")
    days = st.number_input("ìµœê·¼ Nì¼", min_value=1, max_value=365, value=30, step=1)
    max_results = st.slider("ìˆ˜ì§‘í•  ì˜ìƒ ìˆ˜", min_value=10, max_value=200, value=80, step=10)
    region_code = st.text_input("ì§€ì—­ ì½”ë“œ (ì„ íƒ, KR/US/JP ë“±)", "KR").strip().upper()
    font_path = st.text_input("ì›Œë“œí´ë¼ìš°ë“œ í•œê¸€ í°íŠ¸ ê²½ë¡œ(ì„ íƒ)", "")
    st.markdown("---")
    run = st.button("ë°ì´í„° ìˆ˜ì§‘/ë¶„ì„ ì‹¤í–‰", type="primary", use_container_width=True)

# ê¸°ê°„ ê³„ì‚° (UTC ê¸°ì¤€ ISO8601)
now = datetime.now(timezone.utc)
published_after = (now - timedelta(days=int(days))).isoformat()
published_before = now.isoformat()

# ê²°ê³¼ ìºì‹œ ì»¨í…Œì´ë„ˆ
result_area = st.container()

if run:
    try:
        with st.spinner("ê²€ìƒ‰ ì¤‘â€¦"):
            ids = youtube_search(keyword, published_after, published_before, region_code, max_results=max_results)
        if len(ids) == 0:
            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ/ê¸°ê°„/ì§€ì—­ ì½”ë“œë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”.")
        else:
            with st.spinner("ì˜ìƒ ë©”íƒ€ë°ì´í„°/í†µê³„ ìˆ˜ì§‘ ì¤‘â€¦"):
                df = youtube_videos_stats(ids)

            if df.empty:
                st.warning("ìˆ˜ì§‘ëœ í†µê³„ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.success(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê°œ ì˜ìƒ")

                # ìƒë‹¨ KPI
                c1, c2, c3, c4 = st.columns(4)
                with c1:
                    st.metric("ì´ ì¡°íšŒìˆ˜", f"{df['viewCount'].sum():,}")
                with c2:
                    st.metric("í‰ê·  ER(%)", f"{df['ER(%)'].mean():.2f}")
                with c3:
                    st.metric("í‰ê·  ëŒ“ê¸€ ìˆ˜", f"{df['commentCount'].mean():.1f}")
                with c4:
                    st.metric("ë¶„ì„ ê¸°ê°„(ì¼)", f"{days}")

                # í…Œì´ë¸”
                st.subheader("ìƒìœ„ ì˜ìƒ(ER ë‚´ë¦¼ì°¨ìˆœ)")
                df_sorted = df.sort_values(["ER(%)","viewCount"], ascending=[False, False]).reset_index(drop=True)
                st.dataframe(
                    df_sorted[["title","channelTitle","viewCount","likeCount","commentCount","ER(%)","publishedAt","videoId"]],
                    use_container_width=True, height=360
                )

                # ì›Œë“œí´ë¼ìš°ë“œ
                st.subheader("ì›Œë“œí´ë¼ìš°ë“œ (ì œëª©+ì„¤ëª…)")
                freqs = keywords_from_df(df_sorted, topn=120)
                if len(freqs) == 0:
                    st.info("ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë¶ˆìš©ì–´ë¥¼ ì¤„ì´ê±°ë‚˜ ê¸°ê°„/ì˜ìƒ ìˆ˜ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.")
                else:
                    fig = draw_wordcloud(freqs, font_path=font_path if font_path.strip() else None)
                    st.pyplot(fig, clear_figure=True)

                # ë¹ˆë„ ìƒìœ„ í‘œ
                st.subheader("í‚¤ì›Œë“œ ìƒìœ„ ë¹ˆë„")
                top_k = pd.DataFrame(freqs[:30], columns=["keyword","freq"])
                st.dataframe(top_k, use_container_width=True, height=400)

                # ì°¸ì—¬ìœ¨ vs ì¡°íšŒìˆ˜ ì‚°í¬
                st.subheader("ì°¸ì—¬ìœ¨(ER%) vs ì¡°íšŒìˆ˜")
                st.scatter_chart(df_sorted, x="viewCount", y="ER(%)", size="commentCount", color=None)
                st.caption("ë²„ë¸” í¬ê¸°ëŠ” ëŒ“ê¸€ ìˆ˜. ER = (ì¢‹ì•„ìš” + ëŒ“ê¸€) / ì¡°íšŒìˆ˜ Ã— 100")

                # CSV ë‚´ë³´ë‚´ê¸°
                st.download_button(
                    "CSV ë‹¤ìš´ë¡œë“œ",
                    df_sorted.to_csv(index=False).encode("utf-8-sig"),
                    file_name=f"yt_{keyword}_{days}d.csv",
                    mime="text/csv",
                    use_container_width=True
                )

    except Exception as e:
        st.error(f"ì˜¤ë¥˜: {e}")
        st.info("API í‚¤, ê¸°ê°„, ì§€ì—­ ì½”ë“œ, ì¿¼í„° ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# ë„ì›€ë§
with st.expander("ë„ì›€ë§ / ì£¼ì˜ì‚¬í•­"):
    st.markdown("""
- **ER(%)** = (likeCount + commentCount) / viewCount Ã— 100  
  - ì¼ë¶€ ì±„ë„ì€ **ì¢‹ì•„ìš” ìˆ˜ ë¹„ê³µê°œ**ë¡œ 0ìœ¼ë¡œ ì˜¬ ìˆ˜ ìˆì–´ìš”.
- **ì§€ì—­ ì½”ë“œ**ëŠ” ì„ íƒì…ë‹ˆë‹¤. ê¸€ë¡œë²Œ íŠ¸ë Œë“œë¥¼ ë³´ë ¤ë©´ ë¹ˆì¹¸ìœ¼ë¡œ ë‘ì„¸ìš”.
- **ì›Œë“œí´ë¼ìš°ë“œ í•œê¸€ í°íŠ¸**ê°€ í•„ìš”í•˜ë©´ ì‹œìŠ¤í…œ í°íŠ¸(.ttf) ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.  
  - ì˜ˆ: `C:/Windows/Fonts/malgun.ttf` (ë§‘ì€ê³ ë”•)
- ì¿¼í„° ì´ˆê³¼ ì‹œ ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆì–´ìš”. ê¸°ê°„/ìˆ˜ì§‘ëŸ‰ì„ ë‚®ì¶”ì„¸ìš”.
""")
