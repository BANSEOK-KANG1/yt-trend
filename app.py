# =============================
# Imports (Streamlit ëª…ë ¹ ì „)
# =============================
import os
import re
import time
import requests
import streamlit as st
import pandas as pd
import numpy as np
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from datetime import datetime, timedelta, timezone

# =============================
# ë°˜ë“œì‹œ ê°€ì¥ ë¨¼ì € ì‹¤í–‰
# =============================
st.set_page_config(
    page_title="YouTube í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ê¸°",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

# =============================
# í•œê¸€ í°íŠ¸ ë¡œë”© (Cloud ëŒ€ì‘)
# =============================
@st.cache_resource
def load_font():
    font_path = os.path.join("fonts", "Pretendard-Regular.otf")
    font_prop = fm.FontProperties(fname=font_path)

    plt.rcParams["font.family"] = font_prop.get_name()
    plt.rcParams["axes.unicode_minus"] = False
    sns.set(font=font_prop.get_name())

    return font_prop

font_prop = load_font()

# =============================
# ìƒìˆ˜ / ì„¤ì •
# =============================
BASE_URL = "https://www.googleapis.com/youtube/v3"
KST = timezone(timedelta(hours=9))

DEFAULT_STOPWORDS = {
    "ì˜ìƒ", "ë™ì˜ìƒ", "ë¸Œì´ë¡œê·¸", "vlog",
    "the", "a", "to", "of", "in", "on", "for", "and", "is", "are", "with", "from",
    "í•œ", "ê²ƒ", "ìˆ˜", "ì´", "ê·¸", "ì €", "ë°", "ë“±", "ë•Œ", "ë•Œë¬¸",
    "ì˜¤ëŠ˜", "í•˜ë£¨", "ì¼ìƒ",
    "ì±„ë„", "ìœ íŠœë¸Œ", "youtube",
    "shorts", "short",
    "ê³µì‹", "official", "full",
    "2023", "2024", "2025"
}

TOKEN_PATTERN = re.compile(r"[A-Za-zê°€-í£]+")

# =============================
# API Key ë¡œë”©
# =============================
def load_api_key():
    return st.secrets.get("YOUTUBE_API_KEY") or os.getenv("YOUTUBE_API_KEY", "")

API_KEY = load_api_key()

# =============================
# YouTube API Wrapper
# =============================
def yt_get(path, params, sleep=0.0):
    if not API_KEY:
        raise RuntimeError("YouTube API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    params = dict(params)
    params["key"] = API_KEY

    r = requests.get(f"{BASE_URL}/{path}", params=params, timeout=30)
    if sleep:
        time.sleep(sleep)

    if r.status_code != 200:
        raise RuntimeError(f"API ì˜¤ë¥˜ {r.status_code}: {r.text}")

    return r.json()

def youtube_search(keyword, published_after, published_before, region_code, max_results):
    ids, fetched, token = [], 0, None

    while fetched < max_results:
        size = min(50, max_results - fetched)
        params = {
            "part": "id",
            "type": "video",
            "q": keyword,
            "publishedAfter": published_after,
            "publishedBefore": published_before,
            "maxResults": size,
            "order": "relevance"
        }
        if region_code:
            params["regionCode"] = region_code
        if token:
            params["pageToken"] = token

        data = yt_get("search", params, 0.05)
        items = data.get("items", [])

        for it in items:
            vid = it.get("id", {}).get("videoId")
            if vid:
                ids.append(vid)

        fetched += len(items)
        token = data.get("nextPageToken")
        if not token:
            break

    return list(dict.fromkeys(ids))

def youtube_videos_stats(video_ids):
    rows = []

    for i in range(0, len(video_ids), 50):
        chunk = ",".join(video_ids[i:i+50])
        data = yt_get("videos", {
            "part": "snippet,statistics",
            "id": chunk
        }, 0.05)

        for it in data.get("items", []):
            s = it["snippet"]
            stt = it.get("statistics", {})
            rows.append({
                "videoId": it["id"],
                "title": s.get("title", ""),
                "description": s.get("description", ""),
                "channelTitle": s.get("channelTitle", ""),
                "publishedAt": s.get("publishedAt", ""),
                "viewCount": int(stt.get("viewCount", 0)),
                "likeCount": int(stt.get("likeCount", 0)),
                "commentCount": int(stt.get("commentCount", 0)),
            })

    df = pd.DataFrame(rows)
    if not df.empty:
        df["publishedAt"] = pd.to_datetime(df["publishedAt"])
        df["ER(%)"] = np.where(
            df["viewCount"] > 0,
            (df["likeCount"] + df["commentCount"]) / df["viewCount"] * 100,
            0
        ).round(3)

    return df

# =============================
# í…ìŠ¤íŠ¸ ì²˜ë¦¬
# =============================
def tokenize(text):
    return [
        t for t in TOKEN_PATTERN.findall(text.lower())
        if t not in DEFAULT_STOPWORDS and len(t) >= 2
    ]

def keywords_from_df(df, topn=100):
    corpus = []
    for _, r in df.iterrows():
        corpus.extend(tokenize(f"{r['title']} {r['description']}"))
    return Counter(corpus).most_common(topn)

def draw_wordcloud(freqs):
    wc = WordCloud(
        width=900,
        height=500,
        background_color="white",
        font_path=os.path.join("fonts", "Pretendard-Regular.otf"),
        collocations=False
    )
    wc.generate_from_frequencies(dict(freqs))
    fig = plt.figure(figsize=(9, 5))
    plt.imshow(wc)
    plt.axis("off")
    return fig

# =============================
# UI
# =============================
st.title("ğŸ“ˆ ìœ íŠœë¸Œ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ê¸°")
st.caption("í‚¤ì›Œë“œ/ê¸°ê°„ ê¸°ì¤€ìœ¼ë¡œ ì˜ìƒ ì„±ê³¼ì™€ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

with st.sidebar:
    keyword = st.text_input("í‚¤ì›Œë“œ", "ë¸Œì´ë¡œê·¸")
    days = st.number_input("ìµœê·¼ Nì¼", 1, 365, 30)
    max_results = st.slider("ì˜ìƒ ìˆ˜", 10, 200, 80, 10)
    region_code = st.text_input("ì§€ì—­ ì½”ë“œ", "KR").strip().upper()

    if API_KEY:
        st.success("API Key ë¡œë“œ ì™„ë£Œ")
    else:
        st.error("API Key ì—†ìŒ")

    run = st.button("ë¶„ì„ ì‹¤í–‰", type="primary")

now = datetime.now(timezone.utc)
after = (now - timedelta(days=days)).isoformat()
before = now.isoformat()

if run and API_KEY:
    ids = youtube_search(keyword, after, before, region_code, max_results)
    df = youtube_videos_stats(ids)

    if not df.empty:
        st.success(f"{len(df)}ê°œ ì˜ìƒ ë¶„ì„ ì™„ë£Œ")

        c1, c2, c3 = st.columns(3)
        c1.metric("ì´ ì¡°íšŒìˆ˜", f"{df['viewCount'].sum():,}")
        c2.metric("í‰ê·  ER(%)", f"{df['ER(%)'].mean():.2f}")
        c3.metric("í‰ê·  ëŒ“ê¸€ ìˆ˜", f"{df['commentCount'].mean():.1f}")

        st.subheader("ìƒìœ„ ì˜ìƒ")
        st.dataframe(df.sort_values("ER(%)", ascending=False), use_container_width=True)

        st.subheader("ì›Œë“œí´ë¼ìš°ë“œ")
        st.pyplot(draw_wordcloud(keywords_from_df(df)), clear_figure=True)

        st.download_button(
            "CSV ë‹¤ìš´ë¡œë“œ",
            df.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"yt_{keyword}_{days}d.csv",
            mime="text/csv"
        )
