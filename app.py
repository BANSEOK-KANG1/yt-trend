import os
import re
import time
import requests
import streamlit as st
import pandas as pd
import numpy as np
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from datetime import datetime, timedelta, timezone

# âœ… Streamlit ê´€ë ¨ í˜¸ì¶œ ì¤‘ ë°˜ë“œì‹œ ì²« ë²ˆì§¸
st.set_page_config(
    page_title="YouTube í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ê¸°",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

# =============================
# í•œê¸€ í°íŠ¸ ë¡œë”© (set_page_config ì´í›„)
# =============================
def load_font():
    font_path = os.path.join("fonts", "Pretendard-Regular.otf")

    # í°íŠ¸ íŒŒì¼ ì—†ìœ¼ë©´ ëª…í™•í•˜ê²Œ ì•Œë ¤ì£¼ê¸°
    if not os.path.exists(font_path):
        st.error(f"í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {font_path}")
        st.stop()

    font_prop = fm.FontProperties(fname=font_path)

    # matplotlib ì „ì—­ í°íŠ¸ ì„¤ì •
    plt.rcParams["font.family"] = font_prop.get_name()
    plt.rcParams["axes.unicode_minus"] = False

    # seaborn í°íŠ¸ ì„¤ì •
    sns.set(font=font_prop.get_name())

    return font_prop, font_path

# ìºì‹œê°€ í•„ìš”í•˜ë©´ st.cache_resourceë¥¼ 'í•¨ìˆ˜ ë°ì½”ë ˆì´í„°'ë¡œ ì“°ì§€ ë§ê³  ì•„ë˜ì²˜ëŸ¼ ì‚¬ìš©
@st.cache_resource
def get_font_cached():
    return load_font()

font_prop, FONT_PATH = get_font_cached()


# =============================
# ìƒìˆ˜ / ì „ì—­ íŒ¨í„´
# =============================
BASE_URL = "https://www.googleapis.com/youtube/v3"
KST = timezone(timedelta(hours=9))

DEFAULT_STOPWORDS = {
    "ì˜ìƒ", "ë™ì˜ìƒ", "ë¸Œì´ë¡œê·¸", "vlog",
    "the", "a", "to", "of", "in", "on", "for", "and", "is", "are", "with", "from",
    "í•œ", "ê²ƒ", "ìˆ˜", "ì´", "ê·¸", "ì €", "ë°", "ë“±", "ë•Œ", "ë•Œë¬¸",
    "ì˜¤ëŠ˜", "í•˜ë£¨", "ì¼ìƒ",
    "ì±„ë„", "ìœ íŠœë¸Œ", "youtube",
    "shorts", "short",
    "ê³µì‹", "official", "full",
    "2023", "2024", "2025"
}

TOKEN_PATTERN = re.compile(r"[A-Za-zê°€-í£]+")

# =============================
# API í‚¤ ë¡œë”©
# =============================
def load_api_key():
    key_from_secrets = st.secrets.get("YOUTUBE_API_KEY", None)
    if key_from_secrets:
        return key_from_secrets
    return os.getenv("YOUTUBE_API_KEY", "")

API_KEY = load_api_key()

# =============================
# YouTube API í˜¸ì¶œ ìœ í‹¸
# =============================
def yt_get(path, params, sleep=0.0):
    if not API_KEY:
        raise RuntimeError(
            "API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. Streamlit Cloudì˜ 'Manage app â†’ Secrets'ì— "
            'YOUTUBE_API_KEY = "ì‹¤ì œ_API_KEY" í˜•ì‹ìœ¼ë¡œ ë“±ë¡í•˜ì„¸ìš”.'
        )

    final_params = dict(params)
    final_params["key"] = API_KEY
    url = f"{BASE_URL}/{path}"

    r = requests.get(url, params=final_params, timeout=30)
    if sleep:
        time.sleep(sleep)

    if r.status_code != 200:
        raise RuntimeError(f"API ì˜¤ë¥˜ {r.status_code}: {r.text}")
    return r.json()

def youtube_search(keyword, published_after, published_before, region_code, max_results=50):
    ids = []
    fetched = 0
    next_page_token = None

    while fetched < max_results:
        page_size = min(50, max_results - fetched)

        query_params = {
            "part": "id",
            "type": "video",
            "q": keyword,
            "publishedAfter": published_after,
            "publishedBefore": published_before,
            "maxResults": page_size,
            "order": "relevance",
        }
        if region_code:
            query_params["regionCode"] = region_code
        if next_page_token:
            query_params["pageToken"] = next_page_token

        data = yt_get("search", query_params, sleep=0.05)

        items = data.get("items", [])
        for item in items:
            vid = item.get("id", {}).get("videoId")
            if vid:
                ids.append(vid)

        fetched += len(items)
        next_page_token = data.get("nextPageToken")
        if (not next_page_token) or (len(items) == 0):
            break

    return list(dict.fromkeys(ids))

def youtube_videos_stats(video_ids):
    rows = []

    for i in range(0, len(video_ids), 50):
        chunk = ",".join(video_ids[i:i+50])
        data = yt_get(
            "videos",
            {"part": "snippet,statistics,contentDetails", "id": chunk},
            sleep=0.05,
        )

        for it in data.get("items", []):
            s = it.get("snippet", {})
            stats = it.get("statistics", {})
            row = {
                "videoId": it.get("id"),
                "title": s.get("title", ""),
                "description": s.get("description", ""),
                "channelTitle": s.get("channelTitle", ""),
                "publishedAt": s.get("publishedAt", ""),
                "viewCount": int(stats.get("viewCount", 0) or 0),
                "likeCount": int(stats.get("likeCount", 0) or 0),
                "commentCount": int(stats.get("commentCount", 0) or 0),
            }
            rows.append(row)

    df = pd.DataFrame(rows)

    if not df.empty:
        df["publishedAt"] = pd.to_datetime(df["publishedAt"], errors="coerce")
        df["ER(%)"] = np.where(
            df["viewCount"] > 0,
            (df["likeCount"] + df["commentCount"]) / df["viewCount"] * 100.0,
            0.0,
        ).round(3)

    return df

# =============================
# í…ìŠ¤íŠ¸ ì²˜ë¦¬ / í‚¤ì›Œë“œ ì¶”ì¶œ
# =============================
def tokenize(text: str):
    tokens = TOKEN_PATTERN.findall(text.lower())
    return [t for t in tokens if t not in DEFAULT_STOPWORDS and len(t) >= 2]

def keywords_from_df(df, topn=100):
    corpus = []
    for _, row in df.iterrows():
        title_txt = row.get("title", "")
        desc_txt = row.get("description", "")
        corpus.extend(tokenize(f"{title_txt} {desc_txt}"))

    counter = Counter(corpus)
    return counter.most_common(topn)

def draw_wordcloud(freqs, font_path):
    wc = WordCloud(
        width=900,
        height=500,
        background_color="white",
        font_path=font_path,
        collocations=False
    )
    wc.generate_from_frequencies(dict(freqs))

    fig = plt.figure(figsize=(9, 5))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    return fig

# =============================
# UI
# =============================
st.title("ğŸ“ˆ ìœ íŠœë¸Œ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ê¸°")
st.caption("í‚¤ì›Œë“œ/ê¸°ê°„ìœ¼ë¡œ ìƒìœ„ ë™ì˜ìƒì„ ìˆ˜ì§‘í•´ ì›Œë“œí´ë¼ìš°ë“œì™€ ì°¸ì—¬ìœ¨(ER)ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.subheader("ê²€ìƒ‰ ì„¤ì •")

    keyword = st.text_input("í‚¤ì›Œë“œ (ì˜ˆ: ë¸Œì´ë¡œê·¸ / ì˜ì–´ ê°€ëŠ¥)", value="ë¸Œì´ë¡œê·¸")
    days = st.number_input("ìµœê·¼ Nì¼", min_value=1, max_value=365, value=30, step=1)
    max_results = st.slider("ìˆ˜ì§‘í•  ì˜ìƒ ìˆ˜", min_value=10, max_value=200, value=80, step=10)
    region_code = st.text_input("ì§€ì—­ ì½”ë“œ (ì„ íƒ, KR/US/JP ë“±)", value="KR").strip().upper()

    st.markdown("---")

    if API_KEY:
        st.success("YouTube API í‚¤ ë¡œë“œ ì™„ë£Œ")
    else:
        st.error("YouTube API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    run = st.button("ë°ì´í„° ìˆ˜ì§‘/ë¶„ì„ ì‹¤í–‰", type="primary", use_container_width=True)

now_utc = datetime.now(timezone.utc)
published_after = (now_utc - timedelta(days=int(days))).isoformat()
published_before = now_utc.isoformat()

if run:
    if not API_KEY:
        st.warning("API í‚¤ê°€ ì—†ì–´ YouTube API í˜¸ì¶œì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. í‚¤ë¥¼ ì„¤ì •í•œ ë’¤ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        st.stop()

    try:
        with st.spinner("ê²€ìƒ‰ ì¤‘â€¦"):
            ids = youtube_search(
                keyword=keyword,
                published_after=published_after,
                published_before=published_before,
                region_code=region_code,
                max_results=max_results
            )

        if len(ids) == 0:
            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ/ê¸°ê°„/ì§€ì—­ ì½”ë“œë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”.")
        else:
            with st.spinner("ì˜ìƒ ë©”íƒ€ë°ì´í„°/í†µê³„ ìˆ˜ì§‘ ì¤‘â€¦"):
                df = youtube_videos_stats(ids)

            if df.empty:
                st.warning("ìˆ˜ì§‘ëœ í†µê³„ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.success(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê°œ ì˜ìƒ")

                c1, c2, c3, c4 = st.columns(4)
                with c1:
                    st.metric("ì´ ì¡°íšŒìˆ˜", f"{df['viewCount'].sum():,}")
                with c2:
                    st.metric("í‰ê·  ER(%)", f"{df['ER(%)'].mean():.2f}")
                with c3:
                    st.metric("í‰ê·  ëŒ“ê¸€ ìˆ˜", f"{df['commentCount'].mean():.1f}")
                with c4:
                    st.metric("ë¶„ì„ ê¸°ê°„(ì¼)", f"{days}")

                st.subheader("ìƒìœ„ ì˜ìƒ (ER ë‚´ë¦¼ì°¨ìˆœ)")
                df_sorted = df.sort_values(["ER(%)", "viewCount"], ascending=[False, False]).reset_index(drop=True)

                st.dataframe(
                    df_sorted[[
                        "title", "channelTitle", "viewCount",
                        "likeCount", "commentCount", "ER(%)",
                        "publishedAt", "videoId"
                    ]],
                    use_container_width=True,
                    height=360
                )

                st.subheader("ì›Œë“œí´ë¼ìš°ë“œ (ì œëª©+ì„¤ëª… ê¸°ë°˜)")
                freqs = keywords_from_df(df_sorted, topn=120)
                if len(freqs) == 0:
                    st.info("ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë¶ˆìš©ì–´ë¥¼ ì¤„ì´ê±°ë‚˜ ê¸°ê°„/ì˜ìƒ ìˆ˜ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.")
                else:
                    fig = draw_wordcloud(freqs, font_path=FONT_PATH)
                    st.pyplot(fig, clear_figure=True)

                st.subheader("í‚¤ì›Œë“œ ìƒìœ„ ë¹ˆë„")
                top_k = pd.DataFrame(freqs[:30], columns=["keyword", "freq"])
                st.dataframe(top_k, use_container_width=True, height=400)

                st.subheader("ì°¸ì—¬ìœ¨(ER%) vs ì¡°íšŒìˆ˜")
                st.scatter_chart(df_sorted, x="viewCount", y="ER(%)", size="commentCount", color=None)
                st.caption("ë²„ë¸” í¬ê¸°ëŠ” ëŒ“ê¸€ ìˆ˜. ER(%) = (ì¢‹ì•„ìš” ìˆ˜ + ëŒ“ê¸€ ìˆ˜) / ì¡°íšŒìˆ˜ Ã— 100")

                st.download_button(
                    label="CSV ë‹¤ìš´ë¡œë“œ",
                    data=df_sorted.to_csv(index=False).encode("utf-8-sig"),
                    file_name=f"yt_{keyword}_{days}d.csv",
                    mime="text/csv",
                    use_container_width=True
                )

    except Exception as e:
        st.error(f"ì˜¤ë¥˜: {e}")
        st.info("API í‚¤ ì„¤ì •, ê¸°ê°„(days), ì§€ì—­ ì½”ë“œ(KR/US/JP ë“±), ë˜ëŠ” YouTube API ì¿¼í„° ìƒíƒœë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")

with st.expander("ë„ì›€ë§ / ì£¼ì˜ì‚¬í•­"):
    st.markdown(r'''
- **ER(%)** = (likeCount + commentCount) / viewCount Ã— 100  
  - ì¼ë¶€ ì±„ë„ì€ ì¢‹ì•„ìš” ìˆ˜ë¥¼ ìˆ¨ê¸°ê±°ë‚˜ ëŒ“ê¸€ì„ ë§‰ì•„ ë‘˜ ìˆ˜ ìˆì–´ì„œ 0ìœ¼ë¡œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **ì§€ì—­ ì½”ë“œ(regionCode)**  
  - `"KR"` ê°™ì´ ë‘ ê¸€ìì˜ êµ­ê°€ ì½”ë“œë¥¼ ì“°ë©´ ì§€ì—­ë³„ ê²€ìƒ‰ ê²½í–¥ì„ ë” ë°˜ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - ë¹ˆì¹¸ìœ¼ë¡œ ë‘ë©´ ì „ì„¸ê³„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.

- **ì¿¼í„° ì œí•œ**  
  - YouTube Data API v3ì—ëŠ” ì¼ì¼ ì¿¼í„°ê°€ ìˆìŠµë‹ˆë‹¤.
  - ë„ˆë¬´ ë§ì€ ì˜ìƒì„ ì§§ì€ ê¸°ê°„ì—ì„œ ê°€ì ¸ì˜¤ë©´ 403/429 ê³„ì—´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    â†’ ê¸°ê°„(days)ì„ ëŠ˜ë¦¬ê±°ë‚˜, max_resultsë¥¼ ì¤„ì´ì„¸ìš”.
''')
