import os
import re
import time
import requests
import streamlit as st
import pandas as pd
import numpy as np
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from datetime import datetime, timedelta, timezone

# -----------------------------
# ê¸°ë³¸ í˜ì´ì§€ ì„¤ì •
# -----------------------------
st.set_page_config(
    page_title="YouTube í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ê¸°",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

# -----------------------------
# ìƒìˆ˜ / ì „ì—­ íŒ¨í„´
# -----------------------------
BASE_URL = "https://www.googleapis.com/youtube/v3"
KST = timezone(timedelta(hours=9))

# ë¶„ì„ì— í° ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ë“¤(ë¶ˆìš©ì–´)
DEFAULT_STOPWORDS = {
    "ì˜ìƒ", "ë™ì˜ìƒ", "ë¸Œì´ë¡œê·¸", "vlog",
    "the", "a", "to", "of", "in", "on", "for", "and", "is", "are", "with", "from",
    "í•œ", "ê²ƒ", "ìˆ˜", "ì´", "ê·¸", "ì €", "ë°", "ë“±", "ë•Œ", "ë•Œë¬¸",
    "ì˜¤ëŠ˜", "í•˜ë£¨", "ì¼ìƒ",
    "ì±„ë„", "ìœ íŠœë¸Œ", "youtube",
    "shorts", "short",
    "ê³µì‹", "official", "full",
    "2023", "2024", "2025"
}

# í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ í† í° ì¶”ì¶œìš© ì •ê·œì‹
TOKEN_PATTERN = re.compile(r"[A-Za-zê°€-í£]+")

# -----------------------------
# API í‚¤ ë¡œë”© ìœ í‹¸
# -----------------------------
def load_api_key():
    """
    Streamlit secrets > OS í™˜ê²½ë³€ìˆ˜(YOUTUBE_API_KEY) ìˆœì„œë¡œ ë¡œë“œ.
    ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜.
    """
    key_from_secrets = st.secrets.get("YOUTUBE_API_KEY", None)
    if key_from_secrets:
        return key_from_secrets
    return os.getenv("YOUTUBE_API_KEY", "")

API_KEY = load_api_key()


# -----------------------------
# YouTube API í˜¸ì¶œ ìœ í‹¸
# -----------------------------
def yt_get(path, params, sleep=0.0):
    """
    YouTube Data API GET ë˜í¼.
    - API í‚¤ ì£¼ì…
    - ê°„ë‹¨ ì¿¼í„° ë³´í˜¸ìš© sleep
    - ì˜¤ë¥˜ ì‹œ RuntimeError ë°œìƒ
    """
    if not API_KEY:
        raise RuntimeError(
            "API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. Streamlit Cloudì˜ 'Manage app â†’ Secrets'ì— "
            'YOUTUBE_API_KEY = "ì‹¤ì œ_API_KEY" í˜•ì‹ìœ¼ë¡œ ë“±ë¡í•˜ê±°ë‚˜, '
            "ë¡œì»¬ì—ì„œëŠ” OS í™˜ê²½ë³€ìˆ˜ YOUTUBE_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        )

    final_params = dict(params)
    final_params["key"] = API_KEY
    url = f"{BASE_URL}/{path}"

    r = requests.get(url, params=final_params, timeout=30)
    if sleep:
        time.sleep(sleep)

    if r.status_code != 200:
        raise RuntimeError(f"API ì˜¤ë¥˜ {r.status_code}: {r.text}")
    return r.json()


def youtube_search(keyword, published_after, published_before,
                   region_code, max_results=50):
    """
    search.listë¥¼ ì‚¬ìš©í•´ì„œ videoId ëª©ë¡ì„ ëª¨ì€ë‹¤.

    keyword: ê²€ìƒ‰ì–´
    published_after / published_before: ISO8601(UTC) ë¬¸ìì—´
    region_code: "KR", "US" ë“± ì§€ì—­ ì½”ë“œ (ë¹ˆ ë¬¸ìì—´ì´ë©´ ì „ì²´)
    max_results: ìµœì¢…ì ìœ¼ë¡œ ê°€ì ¸ì˜¬ ëª©í‘œ ê°œìˆ˜
    """
    ids = []
    fetched = 0
    next_page_token = None

    while fetched < max_results:
        page_size = min(50, max_results - fetched)

        query_params = {
            "part": "id",
            "type": "video",
            "q": keyword,
            "publishedAfter": published_after,
            "publishedBefore": published_before,
            "maxResults": page_size,
            "order": "relevance",
        }
        # regionCodeëŠ” ì„ íƒ í•­ëª©. ë¹ˆ ë¬¸ìì—´ì´ë©´ ë„£ì§€ ì•ŠëŠ”ë‹¤.
        if region_code:
            query_params["regionCode"] = region_code

        # ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ pageToken ì¶”ê°€
        if next_page_token:
            query_params["pageToken"] = next_page_token

        data = yt_get("search", query_params, sleep=0.05)

        items = data.get("items", [])
        for item in items:
            vid = item.get("id", {}).get("videoId")
            if vid:
                ids.append(vid)

        fetched += len(items)
        next_page_token = data.get("nextPageToken")

        # ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ê±°ë‚˜ ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
        if (not next_page_token) or (len(items) == 0):
            break

    # dict.fromkeys: ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
    return list(dict.fromkeys(ids))


def youtube_videos_stats(video_ids):
    """
    videos.listë¥¼ ì‚¬ìš©í•´ ë©”íƒ€ë°ì´í„°/í†µê³„ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•œë‹¤.
    ë°˜í™˜ ì»¬ëŸ¼ ì˜ˆ:
    - videoId, title, description, channelTitle, publishedAt
    - viewCount, likeCount, commentCount
    - ER(%) = (like+comment)/view * 100
    """
    rows = []

    for i in range(0, len(video_ids), 50):
        chunk = ",".join(video_ids[i:i+50])
        data = yt_get(
            "videos",
            {
                "part": "snippet,statistics,contentDetails",
                "id": chunk,
            },
            sleep=0.05,
        )

        for it in data.get("items", []):
            s = it.get("snippet", {})
            stats = it.get("statistics", {})
            row = {
                "videoId": it.get("id"),
                "title": s.get("title", ""),
                "description": s.get("description", ""),
                "channelTitle": s.get("channelTitle", ""),
                "publishedAt": s.get("publishedAt", ""),
                "viewCount": int(stats.get("viewCount", 0) or 0),
                "likeCount": int(stats.get("likeCount", 0) or 0),
                "commentCount": int(stats.get("commentCount", 0) or 0),
            }
            rows.append(row)

    df = pd.DataFrame(rows)

    if not df.empty:
        # publishedAtì„ ì‹œê³„ì—´ë¡œ ë³€í™˜
        df["publishedAt"] = pd.to_datetime(df["publishedAt"], errors="coerce")

        # ì°¸ì—¬ìœ¨(ER, Engagement Rate) ê³„ì‚°
        df["ER(%)"] = np.where(
            df["viewCount"] > 0,
            (df["likeCount"] + df["commentCount"]) / df["viewCount"] * 100.0,
            0.0,
        ).round(3)

    return df


# -----------------------------
# í…ìŠ¤íŠ¸ ì²˜ë¦¬ / í‚¤ì›Œë“œ ì¶”ì¶œ
# -----------------------------
def tokenize(text: str):
    """
    ì œëª©+ì„¤ëª…ì—ì„œ í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ë§Œ ì¶”ì¶œí•˜ê³ 
    ë¶ˆìš©ì–´ì™€ ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ëŠ” ì œê±°í•œë‹¤.
    """
    tokens = TOKEN_PATTERN.findall(text.lower())
    return [
        t for t in tokens
        if t not in DEFAULT_STOPWORDS and len(t) >= 2
    ]


def keywords_from_df(df, topn=100):
    """
    DataFrameì—ì„œ title+descriptionì„ ëª¨ì•„ í† í°í™”í•˜ê³ 
    ìƒìœ„ ë¹ˆë„ í‚¤ì›Œë“œë¥¼ (ë‹¨ì–´, ë¹ˆë„) ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•œë‹¤.
    """
    corpus = []
    for _, row in df.iterrows():
        title_txt = row.get("title", "")
        desc_txt = row.get("description", "")
        corpus.extend(tokenize(f"{title_txt} {desc_txt}"))

    counter = Counter(corpus)
    return counter.most_common(topn)


def draw_wordcloud(freqs, font_path=None):
    """
    (ë‹¨ì–´, ë¹ˆë„) or Counter ê¸°ë°˜ìœ¼ë¡œ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ê·¸ë¦°ë‹¤.
    font_path: í•œê¸€ í°íŠ¸ ê²½ë¡œ (Cloudì—ëŠ” ê¸°ë³¸ í•œê¸€ í°íŠ¸ê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
    """
    wc = WordCloud(
        width=900,
        height=500,
        background_color="white",
        font_path=font_path if font_path else None,
        collocations=False
    )
    wc.generate_from_frequencies(dict(freqs))

    fig = plt.figure(figsize=(9, 5))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    return fig


# -----------------------------
# UI í—¤ë”
# -----------------------------
st.title("ğŸ“ˆ ìœ íŠœë¸Œ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„ê¸°")
st.caption("í‚¤ì›Œë“œ/ê¸°ê°„ìœ¼ë¡œ ìƒìœ„ ë™ì˜ìƒì„ ìˆ˜ì§‘í•´ ì›Œë“œí´ë¼ìš°ë“œì™€ ì°¸ì—¬ìœ¨(ER)ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

# -----------------------------
# ì‚¬ì´ë“œë°” ì…ë ¥ UI
# -----------------------------
with st.sidebar:
    st.subheader("ê²€ìƒ‰ ì„¤ì •")

    keyword = st.text_input(
        "í‚¤ì›Œë“œ (ì˜ˆ: ë¸Œì´ë¡œê·¸ / ì˜ì–´ ê°€ëŠ¥)",
        value="ë¸Œì´ë¡œê·¸"
    )

    days = st.number_input(
        "ìµœê·¼ Nì¼",
        min_value=1,
        max_value=365,
        value=30,
        step=1
    )

    max_results = st.slider(
        "ìˆ˜ì§‘í•  ì˜ìƒ ìˆ˜",
        min_value=10,
        max_value=200,
        value=80,
        step=10
    )

    region_code = st.text_input(
        "ì§€ì—­ ì½”ë“œ (ì„ íƒ, KR/US/JP ë“±)",
        value="KR"
    ).strip().upper()

    font_path = st.text_input(
        "ì›Œë“œí´ë¼ìš°ë“œ í•œê¸€ í°íŠ¸ ê²½ë¡œ(ì„ íƒ)",
        value=""
    )

    st.markdown("---")

    # API í‚¤ ìƒíƒœ í‘œì‹œ
    if API_KEY:
        st.success("YouTube API í‚¤ ë¡œë“œ ì™„ë£Œ")
    else:
        st.error("YouTube API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.markdown(
            "Streamlit Cloud â†’ **Manage app â†’ Secrets** ì— ì•„ë˜ì²˜ëŸ¼ ë“±ë¡í•˜ì„¸ìš”:\n\n"
            "```toml\n"
            'YOUTUBE_API_KEY = "ì—¬ê¸°ì—_ì‹¤ì œ_API_KEY"\n'
            "```\n\n"
            "ë¡œì»¬(ìœˆë„ìš° PowerShell)ì—ì„œ í…ŒìŠ¤íŠ¸í•  ê²½ìš°:\n\n"
            "```powershell\n"
            'setx YOUTUBE_API_KEY "ì—¬ê¸°ì—_ì‹¤ì œ_API_KEY"\n'
            "```\n\n"
            "ë¡œì»¬(macOS / Linux)ì—ì„œëŠ”:\n\n"
            "```bash\n"
            'export YOUTUBE_API_KEY="ì—¬ê¸°ì—_ì‹¤ì œ_API_KEY"\n'
            "```"
        )

    run = st.button(
        "ë°ì´í„° ìˆ˜ì§‘/ë¶„ì„ ì‹¤í–‰",
        type="primary",
        use_container_width=True
    )

# -----------------------------
# ë¶„ì„ìš© ê¸°ê°„ ê³„ì‚° (UTC ê¸°ì¤€)
# -----------------------------
now_utc = datetime.now(timezone.utc)
published_after = (now_utc - timedelta(days=int(days))).isoformat()
published_before = now_utc.isoformat()

# -----------------------------
# ì‹¤í–‰ ë¡œì§
# -----------------------------
if run:
    # API í‚¤ ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ ë§‰ê³  ì•ˆë‚´
    if not API_KEY:
        st.warning("API í‚¤ê°€ ì—†ì–´ YouTube API í˜¸ì¶œì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. í‚¤ë¥¼ ì„¤ì •í•œ ë’¤ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        st.stop()

    try:
        with st.spinner("ê²€ìƒ‰ ì¤‘â€¦"):
            ids = youtube_search(
                keyword=keyword,
                published_after=published_after,
                published_before=published_before,
                region_code=region_code,
                max_results=max_results
            )

        if len(ids) == 0:
            st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ/ê¸°ê°„/ì§€ì—­ ì½”ë“œë¥¼ ì¡°ì •í•´ë³´ì„¸ìš”.")
        else:
            with st.spinner("ì˜ìƒ ë©”íƒ€ë°ì´í„°/í†µê³„ ìˆ˜ì§‘ ì¤‘â€¦"):
                df = youtube_videos_stats(ids)

            if df.empty:
                st.warning("ìˆ˜ì§‘ëœ í†µê³„ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.success(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(df)}ê°œ ì˜ìƒ")

                # KPI ì¹´ë“œ
                c1, c2, c3, c4 = st.columns(4)
                with c1:
                    st.metric("ì´ ì¡°íšŒìˆ˜", f"{df['viewCount'].sum():,}")
                with c2:
                    st.metric("í‰ê·  ER(%)", f"{df['ER(%)'].mean():.2f}")
                with c3:
                    st.metric("í‰ê·  ëŒ“ê¸€ ìˆ˜", f"{df['commentCount'].mean():.1f}")
                with c4:
                    st.metric("ë¶„ì„ ê¸°ê°„(ì¼)", f"{days}")

                # ìƒìœ„ ì˜ìƒ í…Œì´ë¸” (ER ë‚´ë¦¼ì°¨ìˆœ ìš°ì„ , ê·¸ë‹¤ìŒ ì¡°íšŒìˆ˜)
                st.subheader("ìƒìœ„ ì˜ìƒ (ER ë‚´ë¦¼ì°¨ìˆœ)")
                df_sorted = df.sort_values(
                    ["ER(%)", "viewCount"],
                    ascending=[False, False]
                ).reset_index(drop=True)

                st.dataframe(
                    df_sorted[[
                        "title",
                        "channelTitle",
                        "viewCount",
                        "likeCount",
                        "commentCount",
                        "ER(%)",
                        "publishedAt",
                        "videoId"
                    ]],
                    use_container_width=True,
                    height=360
                )

                # ì›Œë“œí´ë¼ìš°ë“œ
                st.subheader("ì›Œë“œí´ë¼ìš°ë“œ (ì œëª©+ì„¤ëª… ê¸°ë°˜)")
                freqs = keywords_from_df(df_sorted, topn=120)
                if len(freqs) == 0:
                    st.info("ìœ ì˜ë¯¸í•œ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ë¶ˆìš©ì–´ë¥¼ ì¤„ì´ê±°ë‚˜ ê¸°ê°„/ì˜ìƒ ìˆ˜ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.")
                else:
                    fig = draw_wordcloud(
                        freqs,
                        font_path=font_path.strip() if font_path.strip() else None
                    )
                    st.pyplot(fig, clear_figure=True)

                # í‚¤ì›Œë“œ ë¹ˆë„ TOP 30
                st.subheader("í‚¤ì›Œë“œ ìƒìœ„ ë¹ˆë„")
                top_k = pd.DataFrame(freqs[:30], columns=["keyword", "freq"])
                st.dataframe(
                    top_k,
                    use_container_width=True,
                    height=400
                )

                # ì°¸ì—¬ìœ¨ vs ì¡°íšŒìˆ˜ ì‚°í¬
                st.subheader("ì°¸ì—¬ìœ¨(ER%) vs ì¡°íšŒìˆ˜")
                st.scatter_chart(
                    df_sorted,
                    x="viewCount",
                    y="ER(%)",
                    size="commentCount",
                    color=None
                )
                st.caption(
                    "ë²„ë¸” í¬ê¸°ëŠ” ëŒ“ê¸€ ìˆ˜. "
                    "ER(%) = (ì¢‹ì•„ìš” ìˆ˜ + ëŒ“ê¸€ ìˆ˜) / ì¡°íšŒìˆ˜ Ã— 100"
                )

                # CSV ë‹¤ìš´ë¡œë“œ
                st.download_button(
                    label="CSV ë‹¤ìš´ë¡œë“œ",
                    data=df_sorted.to_csv(index=False).encode("utf-8-sig"),
                    file_name=f"yt_{keyword}_{days}d.csv",
                    mime="text/csv",
                    use_container_width=True
                )

    except Exception as e:
        st.error(f"ì˜¤ë¥˜: {e}")
        st.info("API í‚¤ ì„¤ì •, ê¸°ê°„(days), ì§€ì—­ ì½”ë“œ(KR/US/JP ë“±), ë˜ëŠ” YouTube API ì¿¼í„° ìƒíƒœë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")

# -----------------------------
# ë„ì›€ë§ / ì£¼ì˜ì‚¬í•­
# -----------------------------
with st.expander("ë„ì›€ë§ / ì£¼ì˜ì‚¬í•­"):
    st.markdown(r'''
- **ER(%)** = (likeCount + commentCount) / viewCount Ã— 100  
  - ì¼ë¶€ ì±„ë„ì€ ì¢‹ì•„ìš” ìˆ˜ë¥¼ ìˆ¨ê¸°ê±°ë‚˜ ëŒ“ê¸€ì„ ë§‰ì•„ ë‘˜ ìˆ˜ ìˆì–´ì„œ 0ìœ¼ë¡œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **ì§€ì—­ ì½”ë“œ(regionCode)**  
  - `"KR"` ê°™ì´ ë‘ ê¸€ìì˜ êµ­ê°€ ì½”ë“œë¥¼ ì“°ë©´ ì§€ì—­ë³„ ê²€ìƒ‰ ê²½í–¥ì„ ë” ë°˜ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - ë¹ˆì¹¸ìœ¼ë¡œ ë‘ë©´ ì „ì„¸ê³„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.

- **ì›Œë“œí´ë¼ìš°ë“œ í•œê¸€ í°íŠ¸ ê²½ë¡œ**  
  - ì˜ˆ: `C:/Windows/Fonts/malgun.ttf` (ë¡œì»¬ Windows í™˜ê²½ì˜ ë§‘ì€ ê³ ë”•)
  - Streamlit Cloud(Ubuntu ê¸°ë°˜)ì—ì„œëŠ” ê¸°ë³¸ í•œê¸€ í°íŠ¸ê°€ ì—†ì„ ìˆ˜ ìˆì–´, ì§ì ‘ .ttf ê²½ë¡œë¥¼ ì œê³µí•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- **ì¿¼í„° ì œí•œ**  
  - YouTube Data API v3ì—ëŠ” ì¼ì¼ ì¿¼í„°ê°€ ìˆìŠµë‹ˆë‹¤.
  - ë„ˆë¬´ ë§ì€ ì˜ìƒì„ ì§§ì€ ê¸°ê°„ì—ì„œ ê°€ì ¸ì˜¤ë©´ 403/429 ê³„ì—´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    â†’ ê¸°ê°„(days)ì„ ëŠ˜ë¦¬ê±°ë‚˜, max_resultsë¥¼ ì¤„ì´ì„¸ìš”.
''')
